from faster_whisper import WhisperModel
import numpy as np
import time
import torch
import openai

# Initialize the OpenAI API key
api_key = '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'
openai.api_key = api_key

# Audio settings
sampling_rate = 16000
chunk_size = 1024

# Initialize Whisper model
model_size = "base"  # You can use other sizes like "base", "small", "medium", etc.
transcription_model = WhisperModel(model_size, device="cuda", compute_type="float16")

# Initialize the VAD model
VAD_model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=True)
(get_speech_timestamps, _, _, _, _) = utils

# Initialize the timestamp and silence state
last_speech_timestamp = time.time()  # Track the last time speech was detected
silence_threshold = 5  # Threshold in seconds to detect silence

async def transcription(audio_chunk):
    """
    Process an audio chunk using the Whisper model to transcribe the audio.
    """
    # Convert byte data to numpy array
    audio_array = np.frombuffer(audio_chunk, np.int16).astype(np.float32) / 32768.0

    # Run Whisper model on the audio data
    segments, _ = transcription_model.transcribe(audio_array, beam_size=5)

    # Collect transcriptions
    transcriptions = ""
    for segment in segments:
        transcriptions += segment.text

    return transcriptions

def vad(audio_chunk):
    """
    Perform Voice Activity Detection (VAD) on an audio chunk.
    """
    global last_speech_timestamp  # Use global variable to keep track of state
    is_silent = False

    # Convert the input chunk to numpy array
    audio_chunk_np = np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32) / 32768.0

    # Process the chunk to get speech timestamps
    speech_timestamps = get_speech_timestamps(audio_chunk_np, VAD_model, sampling_rate=sampling_rate)
    
    # Check if there is speech in this chunk
    if len(speech_timestamps) > 0:
        last_speech_timestamp = time.time()
        is_silent = False
    else:
        # Check if the silence duration exceeds the threshold
        if time.time() - last_speech_timestamp > silence_threshold:
            print("No audio detected for 5 seconds. Exiting.")
            return True  # Signal to stop the stream

    return False


def open_ai_chatbot(transcriptions):
    """
    Function to interact with the OpenAI GPT-4 model using the given transcriptions.
    
    Parameters:
    transcriptions (list of str): A list of strings representing conversation history or messages.

    Returns:
    str: The response generated by the OpenAI GPT-4 model.
    """
    
    # Create a prompt from the transcriptions
    prompt = "\n".join([f"User: {line}" for line in transcriptions])
    prompt += "\nChatbot:"
    
    try:
        # Generate a response from the OpenAI GPT-4 model
        response = openai.Completion.create(
            model="gpt-4",
            prompt=prompt,
            max_tokens=150,  # Adjust the response length as needed
            temperature=0.7,  # Adjust for creativity and diversity
            top_p=1.0,
            frequency_penalty=0.0,
            presence_penalty=0.0
        )
        
        # Extract and return the response text
        return response.choices[0].text.strip()
    
    except Exception as e:
        # Handle any exceptions
        print(f"An error occurred: {e}")
        return "Sorry, I couldn't process your request."


